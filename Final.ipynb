{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_good_cl_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PoGKQq9_a19C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **OFFENSIVE WORD DETECTION** "
      ]
    },
    {
      "metadata": {
        "id": "-CTo6aKbDYJQ",
        "colab_type": "code",
        "outputId": "28de27e7-8161-4f73-9e40-4b77fc2606cd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "dafa = pd.read_csv('/Users/vivad/Downloads/labeled_data.csv')\n",
        "\n",
        "#<p>The dataset was used in Proceedings of the 11th International AAAI Conference on Weblogs and Social Media</p>\n",
        "#The dataset is obtained from this website\n",
        "#https://data.world/thomasrdavidson/hate-speech-and-offensive-language\n",
        "\n",
        "#<p>Every tweet in this dataset is marked either as hate_speech, offensive_lanuage or neither</p>\n",
        "\n",
        "#We have considered hate_speech and offensive_language as one category and neither as another category\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "dafa = dafa.drop('count', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('hate_speech', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('offensive_language', axis = 1)\n",
        "\n",
        "dafa = dafa.drop('neither', axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "dafa = dafa.drop(dafa.columns[0], axis = 1)\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "y = dafa['class']\n",
        "\n",
        "dafa = dafa.drop('class', axis = 1)\n",
        "\n",
        "#dafa.head(5)\n",
        "\n",
        "#dafa = dafa = dafa.drop('neither', axis = 1)\n",
        "\n",
        "#y.head(5)\n",
        "\n",
        "y = y.replace(to_replace = 1, value = 0)\n",
        "\n",
        "y = y.replace(to_replace = 2, value = 1)\n",
        "\n",
        "#dafa = dafa.drop('class', axis = 1)\n",
        "\n",
        "df = dafa\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], y, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "#clf = MultinomialNB()\n",
        "clf = PassiveAggressiveClassifier()\n",
        "clf.fit(count_train, y_train)\n",
        "pred = clf.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Offensive word detection Accuracy:   %0.3f\" % score)\n",
        "\n",
        "#y_test.head(5)\n",
        "\n",
        "#pred\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==1):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "c = 0\n",
        "for i in pred:\n",
        "    if(i==0):\n",
        "        c+=1\n",
        "#print(c)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Offensive word detection Accuracy:   94.889\n",
            "[[6591  209]\n",
            " [ 209 1170]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_AsNzDsw9mcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJc4IUWDFwJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "TZWDu2P7DYMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pw3XYcILDYMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# FAKE NEWS DETECTION"
      ]
    },
    {
      "metadata": {
        "id": "I74xxyI3DYNS",
        "colab_type": "code",
        "outputId": "2b8b0ce4-93f1-4a08-a6f6-3924f74930e4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/Users/vivad/Downloads/train.tsv', delimiter='\\t')\n",
        "print(df.shape)\n",
        "\n",
        "#This is the Liar, Liar pants on Fire dataset by William Yang Wang\n",
        "#https://arxiv.org/pdf/1705.00648.pdf\n",
        "#This dataset divides the news into six categories - <b>Half-true, mostly-true, true, barely-true, false, pants-fire</b></p>\n",
        "#For our project we have reduced the labesl to two - Fake or Real. This means we have combined similar labels.</p> \n",
        "\n",
        "\n",
        "\n",
        "new = df[df.columns[1:3]]\n",
        "\n",
        "\n",
        "\n",
        "new.columns = ['Label', 'news']\n",
        "\n",
        "\n",
        "\n",
        "new['Label'].unique()\n",
        "\n",
        "#new['Label'] = new['Label'].replace('half-true','true')\n",
        "#new['Label'] = new['Label'].replace('mostly-true','true')\n",
        "#new['Label'] = new['Label'].replace('barely-true','false')\n",
        "new['Label'] = new['Label'].replace('pants-fire','false')\n",
        "\n",
        "train = new.copy()\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/Users/vivad/Downloads/test.tsv', delimiter='\\t')\n",
        "new2 = df[df.columns[1:3]]\n",
        "new2.columns = ['Label', 'news']\n",
        "#new2['Label'] = new2['Label'].replace('half-true','true')\n",
        "#new2['Label'] = new2['Label'].replace('mostly-true','true')\n",
        "#new2['Label'] = new2['Label'].replace('barely-true','false')\n",
        "new2['Label'] = new2['Label'].replace('pants-fire','false')\n",
        "test = new2.copy()\n",
        "\n",
        "frames = [train, test]\n",
        "result = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "new['Label'].value_counts()\n",
        "\n",
        "new1 = new[new.Label=='true'].sample(n=1000).copy()\n",
        "new2 = new[new.Label=='false'].sample(frac=1).copy()\n",
        "frames = [new1, new2]\n",
        "result = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "\n",
        "result['Label'].value_counts()\n",
        "\n",
        "result = result.sample(frac=1)\n",
        "df = result\n",
        "\n",
        "df['Label'].value_counts()\n",
        "\n",
        "\n",
        "\n",
        "y = df['Label']\n",
        "df = df.drop('Label', axis = 1)\n",
        "\n",
        "#df.shape\n",
        "\n",
        "maxacc=0\n",
        "for iii in range(1):\n",
        "    #df = df.sample(frac=1)\n",
        "    #print(df['Label'].value_counts())\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['news'], y, test_size=0.20, random_state=24)\n",
        "    #iii+=9\n",
        "    count_vectorizer = CountVectorizer(stop_words='english')\n",
        "    count_train = count_vectorizer.fit_transform(X_train)\n",
        "    count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "    tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    tfidf_vectorizer.get_feature_names()[-10:]\n",
        "    count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "    count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "    tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "    difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "    #print(count_df.equals(tfidf_df))\n",
        "    count_df.head()\n",
        "\n",
        "    tfidf_df.head()\n",
        "\n",
        "    linear_clf = MultinomialNB()\n",
        "\n",
        "    linear_clf.fit(count_train, y_train)\n",
        "    pred = linear_clf.predict(count_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    score = score * 100\n",
        "    if(score>maxacc):\n",
        "        maxacc=score\n",
        "        print(\"Fake News detection accuracy:   %0.3f\" % score)\n",
        "        print(iii)\n",
        "    cm = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
        "\n",
        "print(cm)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10239, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fake News detection accuracy:   73.794\n",
            "0\n",
            "[[ 44 148]\n",
            " [ 53 522]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-uiueoidDYNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1GBhwFlYDYNj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SARCASM DETECTION"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rKswTh9ZEEd0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#/Users/vivad/Downloads/all/yelp_training_set/yelp_training_set_review.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-QgEHpLDYNt",
        "colab_type": "code",
        "outputId": "a9abf308-5e1c-4d67-e4b9-70f2fb8a2e53",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import csv\n",
        "\n",
        "txt_file1 = b\"/Users/vivad/Downloads/SarcasmData/sarcasm_tweets_gaps.txt\"\n",
        "csv_file1 = b\"/Users/vivad/Downloads/SarcasmData/sarcasm_good.csv\"\n",
        "txt_file2 = b\"/Users/vivad/Downloads/SarcasmData/nonsarcasm_tweets_nogap.txt\"\n",
        "csv_file2 = b\"/Users/vivad/Downloads/SarcasmData/nonsarcasm_good.csv\"\n",
        "in_txt1 = csv.reader(open(txt_file1, \"rt\", encoding='UTF-8'), delimiter = '\\n')\n",
        "out_csv1 = csv.writer(open(csv_file1, 'wt', encoding='UTF-8'))\n",
        "\n",
        "out_csv1.writerows(in_txt1)\n",
        "\n",
        "in_txt2 = csv.reader(open(txt_file2, \"rt\", encoding='UTF-8'), delimiter = '\\n')\n",
        "out_csv2 = csv.writer(open(csv_file2, 'wt', encoding='UTF-8'))\n",
        "\n",
        "out_csv2.writerows(in_txt2)\n",
        "\n",
        "df1 = pd.read_csv('/Users/vivad/Downloads/SarcasmData/sarcasm_good.csv', header=None)\n",
        "df2 = pd.read_csv('/Users/vivad/Downloads/SarcasmData/nonsarcasm_good.csv', header=None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "frames = [df1, df2]\n",
        "df3 = pd.concat(frames, ignore_index=True)\n",
        "#print(df3.shape)\n",
        "df3[\"label\"] =''\n",
        "\n",
        "df3.shape\n",
        "for i in range(2303):\n",
        "    \n",
        "    df3.at[i,'label']=str('sarcasm')\n",
        "for i in range(2303, 3655):\n",
        "    \n",
        "    df3.at[i,'label']=str('non-sarcasm')\n",
        "df3 = df3.rename(columns={ df3.columns[0]: \"text\" })\n",
        "\n",
        "df3.shape\n",
        "for i in range(2303):\n",
        "    \n",
        "    df3.at[i,'label']=str('sarcasm')\n",
        "for i in range(2303, 3655):\n",
        "    \n",
        "    df3.at[i,'label']=str('non-sarcasm')\n",
        "\n",
        "df3.at[3,'label']\n",
        "\n",
        "df3[3650:]\n",
        "\n",
        "##\n",
        "\n",
        "df3 = df3.rename(columns={ df3.columns[0]: \"text\" })\n",
        "\n",
        "df = df3.copy()\n",
        "df = df.sample(frac=1).copy()\n",
        "\n",
        "dforg = df.copy()\n",
        "#print(df.shape)\n",
        "#df = df.set_index('Unnamed: 0')\n",
        "dff = df.copy()\n",
        "df.head()\n",
        "y = df.label\n",
        "#dff = df.drop('title', axis=1)\n",
        "#dff = df.drop('text', axis=1)\n",
        "#dff =dff.drop(df.columns[[0,1,2]], axis=1).copy()\n",
        "#print(dff.head())\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop('label', axis=1)\n",
        "\n",
        "# Splitting the dataset into Training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=2)\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "linear_clf = MultinomialNB()\n",
        "\n",
        "linear_clf.fit(tfidf_train, y_train)\n",
        "pred = linear_clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sarcasm Detection accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['sarcasm', 'non-sarcasm'])\n",
        "\n",
        "print(cm)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sarcasm Detection accuracy:   84.907\n",
            "[[746  22]\n",
            " [125 306]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nAQFzPnXDYOM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1c4E9e27DYOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBdivnwvDYOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BlPTCIaDYOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdyS6NLjDYPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QZFanefTFYu1",
        "outputId": "3c54fd21-20ad-421a-9339-a99d285573bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gX9CeiTmgJk1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yza-hCoHgKJx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "487ESIWvEEeI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn import metrics\n",
        "from keras.models import Sequential, model_from_json\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier as PAC\n",
        "from keras.layers.core import Dropout, Dense, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "df=pd.read_json(\"drive/My Drive/Colab Notebooks/yelp_training_set_review.json\", lines=True)\n",
        "#print(df)\n",
        "df.to_csv(\"drive/My Drive/Colab Notebooks/final_senti_results.csv\")\n",
        "\n",
        "df.shape\n",
        "\n",
        "df['stars'].unique()\n",
        "\n",
        "dff =df.drop(df.columns[[0,1,2, 5,6,7]], axis=1).copy()\n",
        "\n",
        "dff.head()\n",
        "\n",
        "\n",
        "dff['stars'].unique()\n",
        "\n",
        "len(dff)\n",
        "\n",
        "dff['stars'].value_counts()\n",
        "\n",
        "dfpos = dff[dff.stars==5].sample(n=20000)\n",
        "\n",
        "dfpos.head()\n",
        "dfpos['stars'].value_counts()\n",
        "\n",
        "dfneg = dff[dff.stars==1].sample(frac=1)\n",
        "dfneg['stars'].value_counts()\n",
        "\n",
        "frames = [dfpos, dfneg]\n",
        "df3 = pd.concat(frames, ignore_index=True)\n",
        "df4 = df3.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ux4DdVwSDYQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df3['stars'] = df3['stars'].replace(1,'negative')\n",
        "df3['stars'] = df3['stars'].replace(5,'positive')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2okMMLEgEEiG",
        "outputId": "130954b0-7552-4a96-a455-6608682f2846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "dforg = df3.copy()\n",
        "#df3['stars'] = df3['stars'].replace(1,'negative')\n",
        "#df3['stars'] = df3['stars'].replace(5,'positive')\n",
        "df = df3.copy()\n",
        "df = df.sample(n=20000).copy()\n",
        "\n",
        "#print(df.shape)\n",
        "#df = df.set_index('Unnamed: 0')\n",
        "dff = df.copy()\n",
        "df.head()\n",
        "y = df.stars\n",
        "#dff = df.drop('title', axis=1)\n",
        "#dff = df.drop('text', axis=1)\n",
        "#dff =dff.drop(df.columns[[0,1,2]], axis=1).copy()\n",
        "#print(dff.head())\n",
        "\n",
        "\n",
        "\n",
        "df = df.drop('stars', axis=1)\n",
        "\n",
        "# Splitting the dataset into Training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=2)\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_vectorizer.get_feature_names()[-10:]\n",
        "count_vectorizer.get_feature_names()[:10]\n",
        "\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "\n",
        "#print(count_df.equals(tfidf_df))\n",
        "count_df.head()\n",
        "\n",
        "tfidf_df.head()\n",
        "\n",
        "#linear_clf = MultinomialNB()\n",
        "linear_clf = PAC()\n",
        "\n",
        "\n",
        "linear_clf.fit(tfidf_train, y_train)\n",
        "pred = linear_clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sentiment detection using PAC - accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['positive', 'negative'])\n",
        "#print(cm)\n",
        "print(classification_report(y_test, pred))\n",
        "print(\"\\n\\n\\n\\n\")\n",
        "linear_clf = MultinomialNB()\n",
        "#linear_clf = PAC(max_iter=1000)\n",
        "\n",
        "\n",
        "linear_clf.fit(tfidf_train, y_train)\n",
        "pred = linear_clf.predict(tfidf_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score = score * 100\n",
        "print(\"Sentiment detection using MNB - accuracy:   %0.3f\" % score)\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['positive', 'negative'])\n",
        "#print(cm)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment detection using PAC - accuracy:   93.652\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.93      0.93      0.93      3079\n",
            "   positive       0.94      0.94      0.94      3521\n",
            "\n",
            "avg / total       0.94      0.94      0.94      6600\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sentiment detection using MNB - accuracy:   91.652\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.92      0.90      0.91      3079\n",
            "   positive       0.92      0.93      0.92      3521\n",
            "\n",
            "avg / total       0.92      0.92      0.92      6600\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tyrrDP3cDYQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SENTIMENT DETECTION USING RNN"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NYOUjMHVEEiK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = df4.copy()\n",
        "data = data.sample(n=6000).copy()\n",
        "\n",
        "\n",
        "data['Sentiment'] = [1 if x > 4 else 0 for x in data.stars]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6iujD3yXEEl1",
        "outputId": "8ddc6d51-7d19-4aea-b245-3a7e707f55fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(13)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(13)\n",
        "\n",
        "X, y = (data['text'].values, data['Sentiment'].values)\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(X)\n",
        "X_seq = tk.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=100, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.20)\n",
        "\n",
        "batch_size = 300\n",
        "X_train1 = X_train[batch_size:]\n",
        "y_train1 = y_train[batch_size:]\n",
        "X_valid = X_train[:batch_size]\n",
        "y_valid = y_train[:batch_size]\n",
        "\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 100\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "adam = Adam(lr=0.0001)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train1, y_train1 , validation_data=(X_valid, y_valid), batch_size=128, epochs=15, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4500 samples, validate on 300 samples\n",
            "Epoch 1/15\n",
            "4500/4500 [==============================] - 15s 3ms/step - loss: 0.6841 - acc: 0.5538 - val_loss: 0.6751 - val_acc: 0.6100\n",
            "Epoch 2/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.6571 - acc: 0.6778 - val_loss: 0.5511 - val_acc: 0.7467\n",
            "Epoch 3/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.4583 - acc: 0.8167 - val_loss: 0.3003 - val_acc: 0.9000\n",
            "Epoch 4/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.1767 - acc: 0.9431 - val_loss: 0.2591 - val_acc: 0.8967\n",
            "Epoch 5/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.1131 - acc: 0.9687 - val_loss: 0.2314 - val_acc: 0.9000\n",
            "Epoch 6/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0527 - acc: 0.9878 - val_loss: 0.2300 - val_acc: 0.9267\n",
            "Epoch 7/15\n",
            "4500/4500 [==============================] - 11s 3ms/step - loss: 0.0344 - acc: 0.9933 - val_loss: 0.3027 - val_acc: 0.9200\n",
            "Epoch 8/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0388 - acc: 0.9916 - val_loss: 0.2482 - val_acc: 0.9067\n",
            "Epoch 9/15\n",
            "4500/4500 [==============================] - 14s 3ms/step - loss: 0.0542 - acc: 0.9898 - val_loss: 0.3574 - val_acc: 0.9033\n",
            "Epoch 10/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0389 - acc: 0.9929 - val_loss: 0.3596 - val_acc: 0.9000\n",
            "Epoch 11/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0347 - acc: 0.9944 - val_loss: 0.2963 - val_acc: 0.9267\n",
            "Epoch 12/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0318 - acc: 0.9947 - val_loss: 0.2701 - val_acc: 0.9200\n",
            "Epoch 13/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.0177 - acc: 0.9973 - val_loss: 0.5262 - val_acc: 0.8967\n",
            "Epoch 14/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.2730 - acc: 0.9411 - val_loss: 0.5433 - val_acc: 0.8033\n",
            "Epoch 15/15\n",
            "4500/4500 [==============================] - 12s 3ms/step - loss: 0.1059 - acc: 0.9787 - val_loss: 0.2805 - val_acc: 0.9067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb85da70e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fbBfFjf2EEl3",
        "outputId": "ee711fc8-3c33-4a6e-ea91-e7c5c6df0689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy :\"+str(scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy :90.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}